from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq
from transformers.trainer_utils import IntervalStrategy
from peft import LoraConfig, get_peft_model

# 1️⃣ Configure LoRA
lora_config = LoraConfig(
    r=8,                     # Low-rank dimension
    lora_alpha=16,            # Scaling factor
    target_modules=["q_proj", "v_proj"],  # apply LoRA to attention
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

# 2️⃣ Apply LoRA to the pre-trained model. This is crucial for fine-tuning quantized models.
model = get_peft_model(model, lora_config)

# 3️⃣ Prepare training arguments
training_args = TrainingArguments(
    output_dir="lora-chatbot",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=1,
    logging_steps=50,
    save_steps=200,
    # evaluation_strategy=IntervalStrategy.STEPS, # Removed due to TypeError in current environment
    fp16=True,
    save_total_limit=2,
    learning_rate=3e-4
)

# 4️⃣ Data collator to dynamically pad sequences
data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors="pt")

# 5️⃣ Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_formatted,
    eval_dataset=valid_formatted,
    data_collator=data_collator
)