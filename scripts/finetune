from transformers import AutoModelForCausalLM, AutoTokenizer

# 1️⃣ Choose a fully open model
model_name = "TheBloke/vicuna-7B-1.1-HF"  # smaller open Vicuna model

# 2️⃣ Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 3️⃣ Load the model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",   # automatically use GPU
    load_in_8bit=True    # reduces memory usage
)

